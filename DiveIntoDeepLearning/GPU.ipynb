{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提升CPU利用率 1\n",
    "# 主内存->L3->L2->L1->寄存器\n",
    "# --L1访问延迟 0.5ns\n",
    "# --L2访问延迟 7ns\n",
    "# --主内存访问延迟 100ns\n",
    "# 提升时间和空间的内存本地性\n",
    "# 时间:重用数据使他们保持在缓存中\n",
    "# 空间:按序读取数据使得可以预读取\n",
    "# 矩阵按行存储，访问一行比访问一列快\n",
    "# CPU会读取64字节(缓存线)“聪明的读取\n",
    "# 连续地址存储，读取快\n",
    "# 提升CPU利用率 2\n",
    "# 并行计算(多线程)\n",
    "# Intel\n",
    "# 两个超线程公用寄存器(对数据计算密集型无用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.000997781753540039]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(1000000)\n",
    "b=torch.rand(1000000)\n",
    "c=torch.rand(1000000)\n",
    "timer=d2l.Timer()\n",
    "timer.start()\n",
    "c=a+b\n",
    "timer.stop()\n",
    "timer.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.000997781753540039, 7.772660255432129]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer.start()\n",
    "for i in range(1000000):\n",
    "    c[i]=a[i]+b[i]\n",
    "timer.stop()\n",
    "timer.cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 调用有开销 差几万倍\n",
    "# 可以做并行\n",
    "# CPU 架构和优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "# GPC\n",
    "# 上千个核，多线程\n",
    "# GPU 内存带宽大\n",
    "# 内存不大，高带宽的显存贵\n",
    "# GPU 都是简单的数值运算 不需要做多级显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题是硬件的设计方式\n",
    "# 提升GPU利用率\n",
    "# -并行\n",
    "# --使用上千个线程\n",
    "# -内存本地性\n",
    "# --缓存更小，架构简单\n",
    "# -少用控制语句\n",
    "# --支持有限\n",
    "# --同步开销大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要频繁的在CPU和GPU之间传数据\n",
    "# 带宽限制，同步开销\n",
    "\n",
    "# CPU C++ 等高性能语言\n",
    "# --编译器成熟\n",
    "\n",
    "# GPU \n",
    "# -- Nvidia 用 CUDA\n",
    "# -- 编译器和驱动成熟\n",
    "# OpenCL\n",
    "# 质量取决于硬件厂商"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU:可以处理通用计算\n",
    "# 性能优化:读写效率和多线程\n",
    "# GPU:使用更多的小核和更好的内存带宽\n",
    "# 适合大规模并行计算任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更多的芯片\n",
    "\n",
    "# DSP:数字信号处理\n",
    "# 为数字信号处理算法设计:点积,卷积,FFT\n",
    "# 低功耗,高性能\n",
    "# VLIW:Very long instrucyion word\n",
    "# 一条指令上百次乘累加\n",
    "# 编程和调试特别困难\n",
    "# 编译器质量差\n",
    "\n",
    "# FPGA(可编程阵列)\n",
    "# 有大量可以编程的逻辑单元和可配置连接\n",
    "# 可以配置成计算复杂函数\n",
    "# VHDL,Verilog\n",
    "# 通常比通用硬件高效\n",
    "# 工具链差\n",
    "# \"编译\"需要数小时\n",
    "\n",
    "# AI ASIC (便宜)\n",
    "# 深度学习的热门领域\n",
    "# 自己的芯片\n",
    "# Google TPU\n",
    "# 核心 systolic array\n",
    "# 越通用越难 特定应用(硬件设计门槛低)\n",
    "\n",
    "# Systolic Array\n",
    "# 计算单元阵列\n",
    "# 特别适合做矩阵乘法\n",
    "# 设计制造简单\n",
    "# Matrix Multiolication with Systolic Array\n",
    "# 对于一般的矩阵进行切开和填充(矩阵分块)\n",
    "# 匹配SA大小\n",
    "# 批量输入减少延迟\n",
    "# 其他硬件单元处理别的NN操作子(Activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习计算器\n",
    "# 只能跑这个任务\n",
    "# 专业性更强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=灵活性和易用性\n",
    "# y=性能和功耗\n",
    "# x+y=const\n",
    "# tensorflow 跑TPU\n",
    "# 芯片必须要有上面的生态\n",
    "# ASIC XPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012 AlexNet\n",
    "# VGG\n",
    "# Inception\n",
    "# 2016 ResNet\n",
    "# AlexNet Conv_Full 完全不同\n",
    "# Conv 计算密集型(容易优化)\n",
    "# Full 不容易优化\n",
    "# ResNet 停滞\n",
    "# 卷积成熟了\n",
    "# 技术快速发展之后就稳定了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 硬件和软件\n",
    "# 鸡生蛋，蛋生鸡的问题\n",
    "# 硬件定义了软件设计架构和思路\n",
    "# Transformer\n",
    "# 特别适合跑 TPU\n",
    "# TPU内存很大 128G\n",
    "# 模型并行与数据并行\n",
    "# 硬件影响算法\n",
    "# 硬件必须有生态圈 开发生态和研究人员生态\n",
    "# 预测很难\n",
    "# 打包卖 NPU(套餐自带)\n",
    "# 关键在于开发人员使用什么\n",
    "# 深度学习框架不开源\n",
    "# 垄断才会商业化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多GPU并行\n",
    "# 单机多卡并行\n",
    "# 一台机器可以安装多个GPU(1-16)\n",
    "# 在训练和预测的时候，将小批量切分到多个GPU\n",
    "# 目的:加速\n",
    "# 切分方案\n",
    "# --数据并行(性能)\n",
    "# --模型并行(内存)\n",
    "# --通道并行(数据+模型并行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 能用单卡计算时，通常使用数据并行拓展到多卡\n",
    "# 模型并行用在超大模型上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同步SGD n个GPU\n",
    "# 每个worker都是同步计算一个批量 \n",
    "# 假设n个GPU，每个GPU处理b个样本\n",
    "# 那么同步SGD等价于单GPU运行批量大小为nb的SGD\n",
    "# 理想n个GPU有n倍加速\n",
    "# GPU机器架构(传输效率不同)\n",
    "# 性能 \n",
    "# t1=在单个GPU上计算b个样本的梯度时间\n",
    "# 假设m个参数，每个worker每次发送m个样本和梯度\n",
    "# t2=发送和接收所用的时间\n",
    "# 每个批量的计算时间为max(t1,t2)\n",
    "# 选取足够大的b使得t1>t2(t1变大,t2不变,t1>1.3*t2)\n",
    "# 增加b或n导致更大的批量大小\n",
    "# 导致需要更多的计算来得到给定的模型精度\n",
    "# 分布式的问题:GPU性能被收发数据挡住了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系统性能,训练有效性的权衡\n",
    "# 实践时的建议\n",
    "# 1.使用一个大数据集\n",
    "# 2.需要好的GPU-GPU和机器-机器带宽(网线和交换机)\n",
    "# 3.高效的数据读取和预处理\n",
    "# 4.模型需要有好的计算(FLOP)通讯(model size)比\n",
    "# --Inception>ResNet>AlexNet(每个epoch读取多少数据大小)\n",
    "# 5.使用足够大的批量大小来得到好的系统性能\n",
    "# 6.使用高效的优化算法对对应大批量大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布式同步数据并行是多GPU数据并行在多机器上的拓展\n",
    "# 网络通讯通常是瓶颈\n",
    "# 需要注意使用特别大的批量大小时收敛效率\n",
    "# 还有异步，模型并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size 越大 冗余重复的数据越多\n",
    "# 计算梯度数据浪费\n",
    "# batch size 越小 数据多样性高，训练效果好\n",
    "# 相当于 iter 少了\n",
    "# 数据与通讯并行\n",
    "# forward无法并行 \n",
    "# backward 可以边计算边发送\n",
    "# batch size 取决于数据集\n",
    "# ImageNet batch size =100000 都可以\n",
    "# 好的优化算法也可以让batch size 更大\n",
    "# batch size < 10*N N 为数据的类"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('ogb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e823c50ccb644dc52d769ac47d58038dc0ab442baeffc4211a51b80e542be21a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
