{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "\n",
    "net=nn.Sequential(\n",
    "    nn.Linear(20,256),nn.ReLU(),\n",
    "    nn.Linear(256,10),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2307, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 初始化\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "        \n",
    "    # 前向传播\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1183,  0.1793, -0.0641,  0.2207, -0.0670,  0.1596,  0.0288, -0.0552,\n",
       "          0.2332, -0.0858],\n",
       "        [-0.0982,  0.0153, -0.1551,  0.1446, -0.0291, -0.0869, -0.0237,  0.0318,\n",
       "          0.3343,  0.0980]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            # 有序字典 OrderedDict\n",
    "            self._modules[block]=block\n",
    "            \n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1367,  0.0079,  0.0159,  0.2326, -0.0025, -0.0241,  0.0326, -0.0012,\n",
       "         -0.0854,  0.1710],\n",
       "        [-0.1701,  0.1012,  0.0217,  0.2800, -0.1296,  0.0390,  0.1010,  0.0321,\n",
       "         -0.0360,  0.1450]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)\n",
    "        self.linear=nn.Linear(20,20)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.linear(X)\n",
    "        X=F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X=self.linear(X)\n",
    "        while X.abs().sum()>1:\n",
    "            X/=2\n",
    "        return X.sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1224, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedHiddenMLP(\n",
       "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()\n",
    "net.share_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(20,64),nn.ReLU(),\n",
    "            nn.Linear(64,32),nn.ReLU(),\n",
    "        )\n",
    "        self.linear=nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0883, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera=nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数管理\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X=torch.rand(size=(2,4))\n",
    "net(X)\n",
    "net.share_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1038,  0.2160, -0.1391,  0.2499, -0.0145, -0.0202, -0.1730,  0.1404]])), ('bias', tensor([-0.0140]))])\n"
     ]
    }
   ],
   "source": [
    "# 参数访问\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0140], requires_grad=True)\n",
      "tensor([-0.0140])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape)for name,param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0140])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"2.bias\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4,8),nn.ReLU(),\n",
    "        nn.Linear(8,4),nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "def block2():\n",
    "    net=nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f\"block {i}\",block1())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3979],\n",
       "        [0.3979]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet=nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.block 0.0.weight',\n",
       "              tensor([[ 0.3876, -0.0572, -0.3471, -0.0484],\n",
       "                      [ 0.0251,  0.2788,  0.2427, -0.2249],\n",
       "                      [ 0.0743,  0.1438, -0.2229, -0.1171],\n",
       "                      [-0.1282, -0.4655, -0.0473, -0.1753],\n",
       "                      [ 0.4167, -0.4027, -0.1831, -0.0505],\n",
       "                      [-0.2240,  0.0078,  0.3060,  0.0972],\n",
       "                      [ 0.1305,  0.4635, -0.1303,  0.3904],\n",
       "                      [ 0.3158,  0.3299, -0.1738, -0.4543]])),\n",
       "             ('0.block 0.0.bias',\n",
       "              tensor([ 0.0499,  0.1651,  0.1781,  0.3592,  0.1308,  0.3764,  0.4786, -0.3723])),\n",
       "             ('0.block 0.2.weight',\n",
       "              tensor([[ 0.2558,  0.0488, -0.0215,  0.0732,  0.3301, -0.0171,  0.1772, -0.0424],\n",
       "                      [ 0.1130,  0.0296,  0.2470, -0.0065, -0.2356,  0.0540,  0.0213,  0.1732],\n",
       "                      [ 0.0119,  0.0143,  0.1322,  0.2425,  0.1184,  0.1317, -0.2126, -0.0495],\n",
       "                      [-0.2486, -0.1393,  0.2797, -0.1220,  0.3033,  0.1758,  0.0204,  0.1630]])),\n",
       "             ('0.block 0.2.bias',\n",
       "              tensor([ 0.2276,  0.2690,  0.0195, -0.0007])),\n",
       "             ('0.block 1.0.weight',\n",
       "              tensor([[-0.2720,  0.0387,  0.1075, -0.3494],\n",
       "                      [ 0.4911,  0.0567, -0.2451,  0.4755],\n",
       "                      [ 0.3392, -0.0524,  0.1760, -0.2104],\n",
       "                      [-0.1157,  0.2250, -0.2541,  0.3809],\n",
       "                      [ 0.2697, -0.1244, -0.2489, -0.3485],\n",
       "                      [-0.3813, -0.0546,  0.4419, -0.3998],\n",
       "                      [-0.1394, -0.2059, -0.2299, -0.2216],\n",
       "                      [-0.4017,  0.0500, -0.0226, -0.2838]])),\n",
       "             ('0.block 1.0.bias',\n",
       "              tensor([ 0.2868,  0.1228, -0.4071, -0.2005, -0.2280,  0.1188, -0.4083, -0.0217])),\n",
       "             ('0.block 1.2.weight',\n",
       "              tensor([[ 0.1459, -0.2722, -0.0335,  0.2247, -0.2689, -0.0978,  0.0692, -0.0936],\n",
       "                      [ 0.1098, -0.3505, -0.3405, -0.0089,  0.2150,  0.0550, -0.0651,  0.0957],\n",
       "                      [ 0.0671, -0.3105, -0.0061,  0.0761,  0.3277,  0.2949, -0.2390, -0.3156],\n",
       "                      [-0.0044, -0.3089, -0.1826,  0.3388,  0.0947, -0.0572, -0.1048, -0.0757]])),\n",
       "             ('0.block 1.2.bias',\n",
       "              tensor([ 0.0237, -0.3502,  0.2761,  0.1950])),\n",
       "             ('0.block 2.0.weight',\n",
       "              tensor([[ 0.4910, -0.2825,  0.3434, -0.4723],\n",
       "                      [-0.3924,  0.3443, -0.1358,  0.2093],\n",
       "                      [-0.4279, -0.2393,  0.2066, -0.4302],\n",
       "                      [ 0.0938, -0.1705, -0.0932,  0.4736],\n",
       "                      [ 0.0461, -0.4591,  0.4299, -0.2037],\n",
       "                      [ 0.1346, -0.1543, -0.1609, -0.1866],\n",
       "                      [-0.4355,  0.1461, -0.0627,  0.1900],\n",
       "                      [ 0.0290,  0.1412, -0.0118,  0.1490]])),\n",
       "             ('0.block 2.0.bias',\n",
       "              tensor([-0.2845,  0.0056, -0.2820,  0.4646,  0.4448,  0.2841,  0.0788,  0.4551])),\n",
       "             ('0.block 2.2.weight',\n",
       "              tensor([[ 0.1844, -0.1716,  0.0994, -0.3329, -0.0500,  0.0040,  0.3112, -0.0438],\n",
       "                      [ 0.3127, -0.0351, -0.1465,  0.0415,  0.1659, -0.2889,  0.0164, -0.3359],\n",
       "                      [ 0.0466, -0.1699, -0.3035,  0.1940, -0.2979, -0.1815, -0.3014,  0.2072],\n",
       "                      [ 0.1267, -0.1856, -0.2468, -0.1474, -0.0515,  0.0488, -0.3534, -0.1613]])),\n",
       "             ('0.block 2.2.bias',\n",
       "              tensor([-0.0786, -0.2304, -0.1853, -0.0647])),\n",
       "             ('0.block 3.0.weight',\n",
       "              tensor([[ 0.1763, -0.3631,  0.1527,  0.0886],\n",
       "                      [ 0.3354, -0.0750,  0.0821, -0.3434],\n",
       "                      [ 0.3973,  0.3975, -0.4666,  0.0945],\n",
       "                      [-0.1017,  0.1516,  0.2246,  0.4554],\n",
       "                      [-0.4980,  0.0802,  0.3933, -0.1864],\n",
       "                      [-0.2429,  0.2426,  0.2357,  0.2942],\n",
       "                      [-0.3791,  0.4397, -0.3953,  0.4947],\n",
       "                      [ 0.4205, -0.1229,  0.0947,  0.4015]])),\n",
       "             ('0.block 3.0.bias',\n",
       "              tensor([ 0.1487, -0.3929, -0.0828, -0.4167,  0.1960,  0.4493,  0.0250, -0.2734])),\n",
       "             ('0.block 3.2.weight',\n",
       "              tensor([[ 2.8409e-01,  2.5514e-01, -1.2797e-04, -3.3112e-01, -4.6263e-02,\n",
       "                       -2.3388e-01,  1.8493e-01, -2.8645e-01],\n",
       "                      [ 1.2244e-01, -9.0589e-02, -1.6821e-01, -1.2736e-02, -3.5139e-01,\n",
       "                        1.0557e-01, -1.1579e-01, -1.0347e-01],\n",
       "                      [-6.5635e-02,  2.6351e-01, -1.9225e-01, -1.1459e-01, -3.0251e-01,\n",
       "                        2.1963e-01, -5.4137e-02,  2.4901e-01],\n",
       "                      [ 1.6070e-01, -2.3444e-01, -2.7651e-01,  2.8025e-01, -1.5548e-01,\n",
       "                        6.3800e-02,  1.3908e-01, -8.4126e-02]])),\n",
       "             ('0.block 3.2.bias',\n",
       "              tensor([ 0.2024, -0.0831,  0.2295,  0.1869])),\n",
       "             ('1.weight', tensor([[ 0.2758, -0.3623, -0.2824,  0.3557]])),\n",
       "             ('1.bias', tensor([0.3578]))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复杂网络模块化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内置初始化\n",
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        # 或者 isinstance\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0101, -0.0033, -0.0013, -0.0129],\n",
       "         [-0.0083,  0.0019, -0.0084,  0.0080],\n",
       "         [ 0.0014, -0.0041, -0.0034, -0.0008],\n",
       "         [ 0.0030,  0.0005,  0.0127,  0.0141],\n",
       "         [-0.0071,  0.0102, -0.0084, -0.0166],\n",
       "         [-0.0284, -0.0002, -0.0147,  0.0034],\n",
       "         [-0.0064, -0.0113, -0.0029, -0.0078],\n",
       "         [ 0.0011,  0.0131, -0.0061,  0.0194]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(init_normal)\n",
    "net[0].weight.data,net[0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_constant(m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(init_constant)\n",
    "net[0].weight.data, net[0].bias.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "        nn.init.constant_(m.weight,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.7031,  0.5043,  0.3071,  0.6540],\n",
       "         [ 0.0689,  0.2009,  0.3191,  0.0474],\n",
       "         [ 0.1849,  0.5142, -0.1172, -0.6657],\n",
       "         [-0.3467,  0.6593,  0.0093,  0.6041],\n",
       "         [-0.0231,  0.4370,  0.6087,  0.2841],\n",
       "         [ 0.4364,  0.5877, -0.3274,  0.5382],\n",
       "         [-0.1846, -0.0218,  0.5533, -0.2661],\n",
       "         [ 0.3078,  0.5764, -0.6570,  0.4341]]),\n",
       " tensor([[42., 42., 42., 42., 42., 42., 42., 42.]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "net[0].weight.data,net[2].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_init(m):\n",
    "    if isinstance(m,nn.Linear):\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name,param.shape) for name,param in m.named_parameters()][0]\n",
    "        )\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        m.weight.data*=m.weight.data.abs()>=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000, -9.1281,  7.3678, -0.0000],\n",
       "        [-0.0000,  7.3443, -7.0752,  0.0000],\n",
       "        [ 7.8845, -0.0000, -0.0000, -0.0000],\n",
       "        [-0.0000,  0.0000, -5.6070, -0.0000],\n",
       "        [ 0.0000,  5.2102,  0.0000, -6.3700],\n",
       "        [-0.0000, -9.8299, -0.0000, -0.0000],\n",
       "        [-0.0000,  0.0000,  7.0766, -7.2053],\n",
       "        [-0.0000, -0.0000, -6.1195,  0.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(my_init)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, -6.1281, 10.3678,  3.0000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:]+=1\n",
    "net[0].weight.data[0][0]=42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数绑定\n",
    "# 相当于同一个实例\n",
    "shared=nn.Linear(8,8)\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                  shared,nn.ReLU(),shared,\n",
    "                  nn.ReLU(),nn.Linear(8,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "net(X)\n",
    "print(net[2].weight.data==net[4].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "net[2].weight.data[0,0]=100\n",
    "print(net[2].weight.data==net[4].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义层\n",
    "# 自定义层和自定义网络没有区别 因为都是nn.Module的子类\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return X-X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer=CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(nn.Linear(8,128,CenteredLayer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0184, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=net(torch.rand(4,8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units):\n",
    "        super().__init__()\n",
    "        # 必须使用nn.Parameter\n",
    "        self.weight=nn.Parameter(torch.randn(in_units,units))\n",
    "        self.bias=nn.Parameter(torch.randn(units,))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        linear=torch.mm(X,self.weight.data)+self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0934, -1.2295,  0.4299],\n",
       "        [ 1.6728, -0.5149, -0.0973],\n",
       "        [-0.1699, -1.1048, -0.1938],\n",
       "        [ 2.0758, -1.5320,  2.0393],\n",
       "        [ 0.1195,  0.6849,  1.8487]], requires_grad=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense=MyLinear(5,3)\n",
    "dense.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 1.1714],\n",
       "        [1.5484, 0.0000, 1.9976]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense(torch.rand(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=nn.Sequential(MyLinear(64,8),MyLinear(8,1))\n",
    "net(torch.rand(2,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读写文件\n",
    "x=torch.arange(4)\n",
    "torch.save(x,\"x-file\")\n",
    "\n",
    "x2=torch.load(\"x-file\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.zeros(4)\n",
    "torch.save([x,y],\"x-files\")\n",
    "x2,y2=torch.load(\"x-files\")\n",
    "[x2,y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict={\"x\":x,\"y\":y}\n",
    "torch.save(mydict,\"mydict\")\n",
    "mydict2=torch.load(\"mydict\")\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载和保存模型参数\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.output=nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=MLP()\n",
    "X=torch.randn(size=(2,20))\n",
    "Y=net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch不方便把模型定义存下来\n",
    "# 只要存权重\n",
    "# 存权重字典即可\n",
    "torch.save(net.state_dict(),\"mlp.params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone=MLP()\n",
    "clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone=clone(X)\n",
    "Y_clone==Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('ogb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e823c50ccb644dc52d769ac47d58038dc0ab442baeffc4211a51b80e542be21a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
